from langchain.memory import ConversationSummaryMemory
from langchain_core.prompts import PromptTemplate
from langchain_ollama import OllamaLLM

class MemoryBasedQA:
    def __init__(self, model_name: str = "gemma3"):
        self.model_name = model_name

        self.memory = ConversationSummaryMemory(
            llm=OllamaLLM(model=model_name, base_url="http://localhost:11434"),
            memory_key="history",
            return_messages=True
        )

        self.prompt = PromptTemplate.from_template("""
        Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ ØªÙ†Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²Ù…ÛŒÙ†Ù‡ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.  
        Ù„Ø·ÙØ§Ù‹ Ù‡ÛŒÚ† Ø¯Ø§Ù†Ø´ Ø®Ø§Ø±Ø¬ÛŒ ÛŒØ§ ÙØ±Ø¶ÛŒØ§Øª Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù†Ú©Ù†ÛŒØ¯.

        ğŸ”¹ **ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡:**  
        ---------------------  
        {history}  
        ---------------------  

        ğŸ”¹ **Ù¾Ø±Ø³Ø´ Ø¬Ø¯ÛŒØ¯:**  
        {question}  

        ğŸ”¹ **Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…Ø³ØªÙ†Ø¯:**  
        """)

    def ask(self, question: str) -> str:
        context = self.memory.load_memory_variables({}).get("history", "")

        llm = OllamaLLM(model=self.model_name, base_url="http://localhost:11434", temperature=0.1)
        response = llm.invoke(self.prompt.format(history=context, question=question))

        self.memory.save_context({"input": question}, {"output": response})

        return response

    def clear_memory(self):
        """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø­Ø§ÙØ¸Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒØ§ÛŒ"""
        self.memory.clear()
