import hashlib
import json
import os
import sqlite3
from typing import List, Dict

from langchain_chroma import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain_text_splitters import RecursiveCharacterTextSplitter

from FileProcessing.EnhancedDoclingLoader import EnhancedDoclingLoader

DB_PATH = "document_database.db"
VECTOR_STORE_PATH = "chroma_db"


class DocumentProcess:
    def __init__(self, model_name: str = "gemma3",
                 embeddings_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        self.model_name = model_name
        self.embeddings_model = embeddings_model
        self.db_conn = self._init_database()

        self.hf_embeddings = HuggingFaceEmbeddings(
            model_name=embeddings_model,
            model_kwargs={'device': 'cpu'},
            cache_folder="./hf_cache"
        )

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", "!", "?", "ØŒ", "Ø›", " ", ""],
        )

        self.vector_store = Chroma(
            persist_directory=VECTOR_STORE_PATH,
            embedding_function=self.hf_embeddings
        )

        self.prompt = PromptTemplate.from_template("""
                Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ ØªÙ†Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²Ù…ÛŒÙ†Ù‡ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. 
                Ø§Ø² Ù‡ÛŒÚ† Ø¯Ø§Ù†Ø´ Ø®Ø§Ø±Ø¬ÛŒ ÛŒØ§ ÙØ±Ø¶ÛŒØ§Øª Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯.

                ðŸ”¹ **Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡:**  
                ---------------------  
                {context}  
                ---------------------  

                ðŸ”¹ **Ù¾Ø±Ø³Ø´:**  
                {question}  

                ðŸ”¹ **Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…Ø³ØªÙ†Ø¯ (ÙÙ‚Ø· Ø§Ø² Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡):**  
                """)

    @staticmethod
    def _init_database() -> sqlite3.Connection:
        conn = sqlite3.connect(DB_PATH)
        conn.execute('''
        CREATE TABLE IF NOT EXISTS processed_files (
            file_hash TEXT PRIMARY KEY,
            file_path TEXT,
            file_name TEXT,
            file_type TEXT,
            page_count INTEGER,
            processed_at TEXT,
            metadata TEXT
        )
        ''')
        conn.commit()
        return conn

    @staticmethod
    def _calculate_file_hash(file_path: str) -> str:
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()

    def get_file_stats(self, file_path: str) -> Dict:
        file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB
        file_type = os.path.splitext(file_path)[1][1:].lower()
        return {
            "file_size": round(file_size, 2),
            "file_type": file_type,
            "file_name": os.path.basename(file_path)
        }

    def process_documents(self, file_paths: List[str], progress_callback=None) -> Dict:
        new_documents = []
        processed_count = skipped_count = 0
        errors = []

        for i, file_path in enumerate(file_paths):
            if not os.path.exists(file_path):
                errors.append(f"ÙØ§ÛŒÙ„ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {file_path}")
                continue

            file_hash = self._calculate_file_hash(file_path)
            file_stats = self.get_file_stats(file_path)

            cursor = self.db_conn.cursor()
            cursor.execute("SELECT 1 FROM processed_files WHERE file_hash = ?", (file_hash,))
            if cursor.fetchone():
                skipped_count += 1
                if progress_callback:
                    progress_callback(i + 1, len(file_paths), f"Ø±Ø¯ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„ ØªÚ©Ø±Ø§Ø±ÛŒ: {file_stats['file_name']}")
                continue

            try:
                if progress_callback:
                    progress_callback(i + 1, len(file_paths),
                                      f"Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´: {file_stats['file_name']} ({file_stats['file_size']} MB)")

                documents = EnhancedDoclingLoader(file_path).load()

                if not documents:
                    errors.append(f"ÙØ§ÛŒÙ„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª: {file_path}")
                    continue

                page_count = len(documents)
                chunks = self.text_splitter.split_documents(documents)
                new_documents.extend(chunks)

                self.db_conn.execute(
                    "INSERT INTO processed_files VALUES (?, ?, ?, ?, ?, datetime('now'), ?)",
                    (
                        file_hash,
                        file_path,
                        file_stats['file_name'],
                        file_stats['file_type'],
                        page_count,
                        json.dumps(
                            {"language": "mixed", "contains_persian": True, "file_size_mb": file_stats['file_size']})
                    )
                )
                self.db_conn.commit()
                processed_count += 1

            except Exception as e:
                errors.append(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {file_path}: {str(e)}")

        if new_documents:
            if progress_callback:
                progress_callback(len(file_paths), len(file_paths),
                                  f"Ø§ÙØ²ÙˆØ¯Ù† {len(new_documents)} Ù‚Ø·Ø¹Ù‡ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø¯Ø§Ø±ÛŒ...")
            self.vector_store.add_documents(new_documents)

        return {
            "processed": processed_count,
            "skipped": skipped_count,
            "errors": errors,
            "new_chunks": len(new_documents)
        }

    def query(self, question: str, top_k: int = 4, progress_callback=None) -> str:
        try:
            if progress_callback:
                progress_callback(0.3, "Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·...")

            retriever = self.vector_store.as_retriever(search_kwargs={"k": top_k})
            source_docs = retriever.invoke(question)

            if not source_docs:
                return "Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ Ø¨Ø§ Ù¾Ø±Ø³Ø´ Ø´Ù…Ø§ ÛŒØ§ÙØª Ù†Ø´Ø¯."

            context = "\n\n".join(doc.page_content for doc in source_docs)

            if progress_callback:
                progress_callback(0.7, "Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®...")

            llm = OllamaLLM(
                model=self.model_name,
                base_url="http://localhost:11434",
                temperature=0.7
            )
            answer = llm.invoke(self.prompt.format(context=context, question=question))

            sources = "\n".join(
                f"Ù…Ù†Ø¨Ø¹ {i}: {os.path.basename(doc.metadata.get('source', 'Ù…Ù†Ø¨Ø¹ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡'))} (Ù†ÙˆØ¹: {doc.metadata.get('filetype', 'Ù†Ø§Ù…Ø´Ø®Øµ')})"
                for i, doc in enumerate(source_docs, 1)
            )

            if progress_callback:
                progress_callback(1.0, "ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")

            return f"Ù¾Ø§Ø³Ø®: {answer}\n\nÙ…Ù†Ø§Ø¨Ø¹:\n{sources}"

        except Exception as e:
            error_message = f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø±Ø³Ø´: {str(e)}"
            print(error_message)
            return f"Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {error_message}\n\nÙ„Ø·ÙØ§Ù‹ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø³Ø±ÙˆÛŒØ³ Ollama Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§Ø³Øª Ùˆ Ù…Ø¯Ù„ {self.model_name} Ù†ØµØ¨ Ø´Ø¯Ù‡ Ø§Ø³Øª."

    def list_processed_files(self) -> List[Dict]:
        cursor = self.db_conn.cursor()
        cursor.execute(
            "SELECT file_path, file_name, file_type, page_count, processed_at, metadata FROM processed_files")
        return [
            {
                "file_path": row[0],
                "file_name": row[1],
                "file_type": row[2],
                "page_count": row[3],
                "processed_at": row[4],
                "metadata": json.loads(row[5]) if row[5] else {}
            }
            for row in cursor.fetchall()
        ]

    def remove_document(self, file_hash: str) -> bool:
        try:
            cursor = self.db_conn.cursor()
            cursor.execute("SELECT file_path FROM processed_files WHERE file_hash = ?", (file_hash,))
            result = cursor.fetchone()

            if not result:
                return False

            cursor.execute("DELETE FROM processed_files WHERE file_hash = ?", (file_hash,))
            self.db_conn.commit()

            return True
        except Exception as e:
            print(f"Error removing document: {e}")
            return False

    def close(self):
        if hasattr(self, 'db_conn') and self.db_conn:
            self.db_conn.close()
